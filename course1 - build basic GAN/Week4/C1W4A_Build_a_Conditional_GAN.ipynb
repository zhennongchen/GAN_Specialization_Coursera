{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1czVdIlqnImH"
   },
   "source": [
    "# Build a Conditional GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1KD3ZgLs80vY"
   },
   "source": [
    "### Goals\n",
    "In this notebook, you're going to make a conditional GAN in order to generate hand-written images of digits, conditioned on the digit to be generated (the class vector). This will let you choose what digit you want to generate.\n",
    "\n",
    "You'll then do some exploration of the generated images to visualize what the noise and class vectors mean.  \n",
    "\n",
    "### Learning Objectives\n",
    "1.   Learn the technical difference between a conditional and unconditional GAN.\n",
    "2.   Understand the distinction between the class and noise vector in a conditional GAN.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eSJ3lVizIR26"
   },
   "source": [
    "## Getting Started\n",
    "\n",
    "For this assignment, you will be using the MNIST dataset again, but there's nothing stopping you from applying this generator code to produce images of animals conditioned on the species or pictures of faces conditioned on facial characteristics.\n",
    "\n",
    "Note that this assignment requires no changes to the architectures of the generator or discriminator, only changes to the data passed to both. The generator will no longer take `z_dim` as an argument, but  `input_dim` instead, since you need to pass in both the noise and class vectors. In addition to good variable naming, this also means that you can use the generator and discriminator code you have previously written with different parameters.\n",
    "\n",
    "You will begin by importing the necessary libraries and building the generator and discriminator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WbRUCE0TK2fV"
   },
   "source": [
    "#### Packages and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfkorNJrnmNO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "torch.manual_seed(0) # Set for our testing purposes, please do not change!\n",
    "\n",
    "def show_tensor_images(image_tensor, num_images=25, size=(1, 28, 28), nrow=5, show=True):\n",
    "    '''\n",
    "    Function for visualizing images: Given a tensor of images, number of images, and\n",
    "    size per image, plots and prints the images in an uniform grid.\n",
    "    '''\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P1A1M6kpnfxw"
   },
   "source": [
    "#### Generator and Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvO7h0LYnEJZ"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generator Class\n",
    "    Values:\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "              (MNIST is black-and-white, so 1 channel is your default)\n",
    "        hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, input_dim=10, im_chan=1, hidden_dim=64):\n",
    "        super(Generator, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        # Build the neural network\n",
    "        self.gen = nn.Sequential(\n",
    "            self.make_gen_block(input_dim, hidden_dim * 4),\n",
    "            self.make_gen_block(hidden_dim * 4, hidden_dim * 2, kernel_size=4, stride=1),\n",
    "            self.make_gen_block(hidden_dim * 2, hidden_dim),\n",
    "            self.make_gen_block(hidden_dim, im_chan, kernel_size=4, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a generator block of DCGAN;\n",
    "        a transposed convolution, a batchnorm (except in the final layer), and an activation.\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.Tanh(),\n",
    "            )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        '''\n",
    "        Function for completing a forward pass of the generator: Given a noise tensor, \n",
    "        returns generated images.\n",
    "        Parameters:\n",
    "            noise: a noise tensor with dimensions (n_samples, input_dim)\n",
    "        '''\n",
    "        x = noise.view(len(noise), self.input_dim, 1, 1)\n",
    "        return self.gen(x)\n",
    "\n",
    "def get_noise(n_samples, input_dim, device='cpu'):\n",
    "    '''\n",
    "    Function for creating noise vectors: Given the dimensions (n_samples, input_dim)\n",
    "    creates a tensor of that shape filled with random numbers from the normal distribution.\n",
    "    Parameters:\n",
    "        n_samples: the number of samples to generate, a scalar\n",
    "        input_dim: the dimension of the input vector, a scalar\n",
    "        device: the device type\n",
    "    '''\n",
    "    return torch.randn(n_samples, input_dim, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r9fScH98nkYH"
   },
   "source": [
    "#### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aA4AxGnmpuPq"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminator Class\n",
    "    Values:\n",
    "      im_chan: the number of channels in the images, fitted for the dataset used, a scalar\n",
    "            (MNIST is black-and-white, so 1 channel is your default)\n",
    "      hidden_dim: the inner dimension, a scalar\n",
    "    '''\n",
    "    def __init__(self, im_chan=1, hidden_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.disc = nn.Sequential(\n",
    "            self.make_disc_block(im_chan, hidden_dim),\n",
    "            self.make_disc_block(hidden_dim, hidden_dim * 2),\n",
    "            self.make_disc_block(hidden_dim * 2, 1, final_layer=True),\n",
    "        )\n",
    "\n",
    "    def make_disc_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n",
    "        '''\n",
    "        Function to return a sequence of operations corresponding to a discriminator block of the DCGAN; \n",
    "        a convolution, a batchnorm (except in the final layer), and an activation (except in the final layer).\n",
    "        Parameters:\n",
    "            input_channels: how many channels the input feature representation has\n",
    "            output_channels: how many channels the output feature representation should have\n",
    "            kernel_size: the size of each convolutional filter, equivalent to (kernel_size, kernel_size)\n",
    "            stride: the stride of the convolution\n",
    "            final_layer: a boolean, true if it is the final layer and false otherwise \n",
    "                      (affects activation and batchnorm)\n",
    "        '''\n",
    "        if not final_layer:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                nn.LeakyReLU(0.2, inplace=True),\n",
    "            )\n",
    "        else:\n",
    "            return nn.Sequential(\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n",
    "            )\n",
    "\n",
    "    def forward(self, image):\n",
    "        '''\n",
    "        Function for completing a forward pass of the discriminator: Given an image tensor, \n",
    "        returns a 1-dimension tensor representing fake/real.\n",
    "        Parameters:\n",
    "            image: a flattened image tensor with dimension (im_chan)\n",
    "        '''\n",
    "        disc_pred = self.disc(image)\n",
    "        return disc_pred.view(len(disc_pred), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CSBva5ffM5KT"
   },
   "source": [
    "## Class Input\n",
    "\n",
    "In conditional GANs, the input vector for the generator will also need to include the class information. The class is represented using a one-hot encoded vector where its length is the number of classes and each index represents a class. The vector is all 0's and a 1 on the chosen class. Given the labels of multiple images (e.g. from a batch) and number of classes, please create one-hot vectors for each label. There is a class within the PyTorch functional library that can help you.\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">get_one_hot_labels</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   This code can be done in one line.\n",
    "2.   The documentation for [F.one_hot](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.one_hot) may be helpful.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "81_1g6odOeV5"
   },
   "outputs": [],
   "source": [
    "# UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_one_hot_labels\n",
    "\n",
    "import torch.nn.functional as F\n",
    "def get_one_hot_labels(labels, n_classes):\n",
    "    '''\n",
    "    Function for creating one-hot vectors for the labels, returns a tensor of shape (?, num_classes).\n",
    "    Parameters:\n",
    "        labels: tensor of labels from the dataloader, size (?)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    return F.one_hot(labels, n_classes)\n",
    "    #### END CODE HERE ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLM5a64HWeqX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "assert (\n",
    "    get_one_hot_labels(\n",
    "        labels=torch.Tensor([[0, 2, 1]]).long(),\n",
    "        n_classes=3\n",
    "    ).tolist() == \n",
    "    [[\n",
    "      [1, 0, 0], \n",
    "      [0, 0, 1], \n",
    "      [0, 1, 0]\n",
    "    ]]\n",
    ")\n",
    "# Check that the device of get_one_hot_labels matches the input device\n",
    "if torch.cuda.is_available():\n",
    "    assert str(get_one_hot_labels(torch.Tensor([[0]]).long().cuda(), 1).device).startswith(\"cuda\")\n",
    "    \n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-DSarQYONAxR"
   },
   "source": [
    "Next, you need to be able to concatenate the one-hot class vector to the noise vector before giving it to the generator. You will also need to do this when adding the class channels to the discriminator.\n",
    "\n",
    "To do this, you will need to write a function that combines two vectors. Remember that you need to ensure that the vectors are the same type: floats. Again, you can look to the PyTorch library for help.\n",
    "<details>\n",
    "<summary>\n",
    "<font size=\"3\" color=\"green\">\n",
    "<b>Optional hints for <code><font size=\"4\">combine_vectors</font></code></b>\n",
    "</font>\n",
    "</summary>\n",
    "\n",
    "1.   This code can also be written in one line.\n",
    "2.   The documentation for [torch.cat](https://pytorch.org/docs/master/generated/torch.cat.html) may be helpful.\n",
    "3.   Specifically, you might want to look at what the `dim` argument of `torch.cat` does.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XbwITPc0M6uh"
   },
   "outputs": [],
   "source": [
    "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: combine_vectors\n",
    "def combine_vectors(x, y):\n",
    "    '''\n",
    "    Function for combining two vectors with shapes (n_samples, ?) and (n_samples, ?).\n",
    "    Parameters:\n",
    "      x: (n_samples, ?) the first vector. \n",
    "        In this assignment, this will be the noise vector of shape (n_samples, z_dim), \n",
    "        but you shouldn't need to know the second dimension's size.\n",
    "      y: (n_samples, ?) the second vector.\n",
    "        Once again, in this assignment this will be the one-hot class vector \n",
    "        with the shape (n_samples, n_classes), but you shouldn't assume this in your code.\n",
    "    '''\n",
    "    # Note: Make sure this function outputs a float no matter what inputs it receives\n",
    "    #### START CODE HERE ####\n",
    "    combined = torch.cat((x.to(torch.float),y.to(torch.float)),1)\n",
    "    #### END CODE HERE ####\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OwpMxamzXcbA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "combined = combine_vectors(torch.tensor([[1, 2], [3, 4]]), torch.tensor([[5, 6], [7, 8]]))\n",
    "if torch.cuda.is_available():\n",
    "    # Check that it doesn't break with cuda\n",
    "    cuda_check = combine_vectors(torch.tensor([[1, 2], [3, 4]]).cuda(), torch.tensor([[5, 6], [7, 8]]).cuda())\n",
    "    assert str(cuda_check.device).startswith(\"cuda\")\n",
    "# Check exact order of elements\n",
    "assert torch.all(combined == torch.tensor([[1, 2, 5, 6], [3, 4, 7, 8]]))\n",
    "# Tests that items are of float type\n",
    "assert (type(combined[0][0].item()) == float)\n",
    "# Check shapes\n",
    "combined = combine_vectors(torch.randn(1, 4, 5), torch.randn(1, 8, 5));\n",
    "assert tuple(combined.shape) == (1, 12, 5)\n",
    "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12).long()).shape) == (1, 30, 12)\n",
    "# Check that the float transformation doesn't happen after the inputs are concatenated\n",
    "assert tuple(combine_vectors(torch.randn(1, 10, 12).long(), torch.randn(1, 20, 12)).shape) == (1, 30, 12)\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Sh98CdJXY_SB"
   },
   "source": [
    "## Training\n",
    "Now you can start to put it all together!\n",
    "First, you will define some new parameters:\n",
    "\n",
    "*   mnist_shape: the number of pixels in each MNIST image, which has dimensions 28 x 28 and one channel (because it's black-and-white) so 1 x 28 x 28\n",
    "*   n_classes: the number of classes in MNIST (10, since there are the digits from 0 to 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGRk6NB4ZqJ8"
   },
   "outputs": [],
   "source": [
    "mnist_shape = (1, 28, 28)\n",
    "n_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-cw3x2kHZd6L"
   },
   "source": [
    "And you also include the same parameters from previous assignments:\n",
    "\n",
    "  *   criterion: the loss function\n",
    "  *   n_epochs: the number of times you iterate through the entire dataset when training\n",
    "  *   z_dim: the dimension of the noise vector\n",
    "  *   display_step: how often to display/visualize the images\n",
    "  *   batch_size: the number of images per forward/backward pass\n",
    "  *   lr: the learning rate\n",
    "  *   device: the device type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3GZt2cxlZRQw"
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss() # This loss combines a Sigmoid layer and the BCELoss in one single class. \n",
    "# The sigmoid activation should be applied in both cases.\n",
    "# While nn.BCEWithLogitsLoss will apply it internally for you, you should add it manually if you are using nn.BCELoss.\n",
    "n_epochs = 200\n",
    "z_dim = 64\n",
    "display_step = 500\n",
    "batch_size = 128\n",
    "lr = 0.0002\n",
    "device = 'cuda'\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    MNIST('.', download=False, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6sxYaNRbRjB"
   },
   "source": [
    "Then, you can initialize your generator, discriminator, and optimizers. To do this, you will need to update the input dimensions for both models. For the generator, you will need to calculate the size of the input vector; recall that for conditional GANs, the generator's input is the noise vector concatenated with the class vector. For the discriminator, you need to add a channel for every class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcTP43grUD5z"
   },
   "outputs": [],
   "source": [
    "# UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED FUNCTION: get_input_dimensions\n",
    "def get_input_dimensions(z_dim, mnist_shape, n_classes):\n",
    "    '''\n",
    "    Function for getting the size of the conditional input dimensions \n",
    "    from z_dim, the image shape, and number of classes.\n",
    "    Parameters:\n",
    "        z_dim: the dimension of the noise vector, a scalar\n",
    "        mnist_shape: the shape of each MNIST image as (C, W, H), which is (1, 28, 28)\n",
    "        n_classes: the total number of classes in the dataset, an integer scalar\n",
    "                (10 for MNIST)\n",
    "    Returns: \n",
    "        generator_input_dim: the input dimensionality of the conditional generator, \n",
    "                          which takes the noise and class vectors\n",
    "        discriminator_im_chan: the number of input channels to the discriminator\n",
    "                            (e.g. C x 28 x 28 for MNIST)\n",
    "    '''\n",
    "    #### START CODE HERE ####\n",
    "    generator_input_dim = z_dim + n_classes\n",
    "    discriminator_im_chan = mnist_shape[0] + n_classes\n",
    "    #### END CODE HERE ####\n",
    "    return generator_input_dim, discriminator_im_chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YouJSJ5cVJvD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "def test_input_dims():\n",
    "    gen_dim, disc_dim = get_input_dimensions(23, (12, 23, 52), 9)\n",
    "    assert gen_dim == 32\n",
    "    assert disc_dim == 21\n",
    "test_input_dims()\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXptQZcwrBrq"
   },
   "outputs": [],
   "source": [
    "generator_input_dim, discriminator_im_chan = get_input_dimensions(z_dim, mnist_shape, n_classes)\n",
    "\n",
    "gen = Generator(input_dim=generator_input_dim).to(device)\n",
    "gen_opt = torch.optim.Adam(gen.parameters(), lr=lr)\n",
    "disc = Discriminator(im_chan=discriminator_im_chan).to(device)\n",
    "disc_opt = torch.optim.Adam(disc.parameters(), lr=lr)\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "gen = gen.apply(weights_init)\n",
    "disc = disc.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EfcEHPwCQYLY"
   },
   "source": [
    "Now to train, you would like both your generator and your discriminator to know what class of image should be generated. There are a few locations where you will need to implement code.\n",
    "\n",
    "For example, if you're generating a picture of the number \"1\", you would need to:\n",
    "  \n",
    "1.   Tell that to the generator, so that it knows it should be generating a \"1\"\n",
    "2.   Tell that to the discriminator, so that it knows it should be looking at a \"1\". If the discriminator is told it should be looking at a 1 but sees something that's clearly an 8, it can guess that it's probably fake\n",
    "\n",
    "There are no explicit unit tests here -- if this block of code runs and you don't change any of the other variables, then you've done it correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pB3hUwWTbVJC"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31df38420444054a7c01b3678e3e5b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=469.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-78-11ddd0cade7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Dataloader returns the batches and the labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mcur_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Flatten the batch of real images from the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1104\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1105\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m             \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# UNQ_C4 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
    "# GRADED CELL\n",
    "cur_step = 0\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "\n",
    "#UNIT TEST NOTE: Initializations needed for grading\n",
    "noise_and_labels = False\n",
    "fake = False\n",
    "\n",
    "fake_image_and_labels = False\n",
    "real_image_and_labels = False\n",
    "disc_fake_pred = False\n",
    "disc_real_pred = False\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Dataloader returns the batches and the labels\n",
    "    for real, labels in tqdm(dataloader):\n",
    "        cur_batch_size = len(real)\n",
    "        # Flatten the batch of real images from the dataset\n",
    "        real = real.to(device)\n",
    "\n",
    "        one_hot_labels = get_one_hot_labels(labels.to(device), n_classes)  # shape = [batch_size, n_classes]\n",
    "     \n",
    "        image_one_hot_labels = one_hot_labels[:, :, None, None]\n",
    " \n",
    "        image_one_hot_labels = image_one_hot_labels.repeat(1, 1, mnist_shape[1], mnist_shape[2]) \n",
    "        # shape = [batch_size, n_classes, image_x_size, image_y_size]\n",
    "    \n",
    "        \n",
    "    \n",
    "        ### Update discriminator ###\n",
    "        # Zero out the discriminator gradients\n",
    "        disc_opt.zero_grad()\n",
    "        # Get noise corresponding to the current batch_size \n",
    "        fake_noise = get_noise(cur_batch_size, z_dim, device=device)\n",
    "        \n",
    "        # Now you can get the images from the generator\n",
    "        # Steps: 1) Combine the noise vectors and the one-hot labels for the generator\n",
    "        #        2) Generate the conditioned fake images\n",
    "       \n",
    "        #### START CODE HERE ####\n",
    "        noise_and_labels = combine_vectors(fake_noise, one_hot_labels)  #for the ith real image with class X, the ith generated image also has class X\n",
    "        fake = gen(noise_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        # Make sure that enough images were generated\n",
    "        assert len(fake) == len(real)\n",
    "        # Check that correct tensors were combined\n",
    "        assert tuple(noise_and_labels.shape) == (cur_batch_size, fake_noise.shape[1] + one_hot_labels.shape[1])\n",
    "        # It comes from the correct generator\n",
    "        assert tuple(fake.shape) == (len(real), 1, 28, 28)\n",
    "\n",
    "        # Now you can get the predictions from the discriminator\n",
    "        # Steps: 1) Create the input for the discriminator\n",
    "        #           a) Combine the fake images with image_one_hot_labels, \n",
    "        #              remember to detach the generator (.detach()) so you do not backpropagate through it\n",
    "        #           b) Combine the real images with image_one_hot_labels\n",
    "        #        2) Get the discriminator's prediction on the fakes as disc_fake_pred\n",
    "        #        3) Get the discriminator's prediction on the reals as disc_real_pred\n",
    "        \n",
    "        #### START CODE HERE ####\n",
    "        fake_image_and_labels = combine_vectors(fake.detach(),image_one_hot_labels)\n",
    "        real_image_and_labels = combine_vectors(real, image_one_hot_labels)\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        disc_real_pred = disc(real_image_and_labels)\n",
    "        #### END CODE HERE ####\n",
    "        \n",
    "        # Make sure shapes are correct \n",
    "        assert tuple(fake_image_and_labels.shape) == (len(real), fake.detach().shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n",
    "        assert tuple(real_image_and_labels.shape) == (len(real), real.shape[1] + image_one_hot_labels.shape[1], 28 ,28)\n",
    "        # Make sure that enough predictions were made\n",
    "        assert len(disc_real_pred) == len(real)\n",
    "        # Make sure that the inputs are different\n",
    "        assert torch.any(fake_image_and_labels != real_image_and_labels)\n",
    "        # Shapes must match\n",
    "        assert tuple(fake_image_and_labels.shape) == tuple(real_image_and_labels.shape)\n",
    "        assert tuple(disc_fake_pred.shape) == tuple(disc_real_pred.shape)\n",
    "        \n",
    "        \n",
    "        disc_fake_loss = criterion(disc_fake_pred, torch.zeros_like(disc_fake_pred))\n",
    "        disc_real_loss = criterion(disc_real_pred, torch.ones_like(disc_real_pred)) # take images + labels from real data, since pre-labeled, always correct\n",
    "        disc_loss = (disc_fake_loss + disc_real_loss) / 2\n",
    "        disc_loss.backward(retain_graph=True)\n",
    "        disc_opt.step() \n",
    "\n",
    "        # Keep track of the average discriminator loss\n",
    "        discriminator_losses += [disc_loss.item()]\n",
    "\n",
    "        ### Update generator ###\n",
    "        # Zero out the generator gradients\n",
    "        gen_opt.zero_grad()\n",
    "\n",
    "        fake_image_and_labels = combine_vectors(fake, image_one_hot_labels)\n",
    "        # This will error if you didn't concatenate your labels to your image correctly\n",
    "        disc_fake_pred = disc(fake_image_and_labels)\n",
    "        gen_loss = criterion(disc_fake_pred, torch.ones_like(disc_fake_pred))\n",
    "        gen_loss.backward()\n",
    "        gen_opt.step()\n",
    "\n",
    "        # Keep track of the generator losses\n",
    "        generator_losses += [gen_loss.item()]\n",
    "        #\n",
    "\n",
    "        if cur_step % display_step == 0 and cur_step > 0:\n",
    "            gen_mean = sum(generator_losses[-display_step:]) / display_step\n",
    "            disc_mean = sum(discriminator_losses[-display_step:]) / display_step\n",
    "            print(f\"Epoch {epoch}, step {cur_step}: Generator loss: {gen_mean}, discriminator loss: {disc_mean}\")\n",
    "            show_tensor_images(fake)\n",
    "            show_tensor_images(real)\n",
    "            step_bins = 20\n",
    "            x_axis = sorted([i * step_bins for i in range(len(generator_losses) // step_bins)] * step_bins)\n",
    "            num_examples = (len(generator_losses) // step_bins) * step_bins\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(generator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Generator Loss\"\n",
    "            )\n",
    "            plt.plot(\n",
    "                range(num_examples // step_bins), \n",
    "                torch.Tensor(discriminator_losses[:num_examples]).view(-1, step_bins).mean(1),\n",
    "                label=\"Discriminator Loss\"\n",
    "            )\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif cur_step == 0:\n",
    "            print(\"Congratulations! If you've gotten here, it's working. Please let this train until you're happy with how the generated numbers look, and then go on to the exploration!\")\n",
    "        cur_step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jhcHh4Gzh1Zo"
   },
   "source": [
    "## Exploration\n",
    "You can do a bit of exploration now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sdJVosOXBSha"
   },
   "outputs": [],
   "source": [
    "# Before you explore, you should put the generator\n",
    "# in eval mode, both in general and so that batch norm\n",
    "# doesn't cause you issues and is using its eval statistics\n",
    "gen = gen.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "838nSeholwPk"
   },
   "source": [
    "#### Changing the Class Vector\n",
    "You can generate some numbers with your new model! You can add interpolation as well to make it more interesting.\n",
    "\n",
    "So starting from a image, you will produce intermediate images that look more and more like the ending image until you get to the final image. Your're basically morphing one image into another. You can choose what these two images will be using your conditional GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wHLOyAzfh3YY"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "### Change me! ###\n",
    "n_interpolation = 11 # Choose the interpolation: how many intermediate images you want + 2 (for the start and end image)\n",
    "interpolation_noise = get_noise(1, z_dim, device=device).repeat(n_interpolation, 1)\n",
    "\n",
    "def interpolate_class(first_number, second_number):\n",
    "    first_label = get_one_hot_labels(torch.Tensor([first_number]).long(), n_classes)\n",
    "    second_label = get_one_hot_labels(torch.Tensor([second_number]).long(), n_classes)\n",
    "\n",
    "    # Calculate the interpolation vector between the two labels\n",
    "    percent_second_label = torch.linspace(0, 1, n_interpolation)[:, None]\n",
    "    interpolation_labels = first_label * (1 - percent_second_label) + second_label * percent_second_label\n",
    "\n",
    "    # Combine the noise and the labels\n",
    "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_labels.to(device))\n",
    "    fake = gen(noise_and_labels)\n",
    "    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n",
    "\n",
    "### Change me! ###\n",
    "start_plot_number = 3 # Choose the start digit\n",
    "### Change me! ###\n",
    "end_plot_number = 6 # Choose the end digit\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "interpolate_class(start_plot_number, end_plot_number)\n",
    "_ = plt.axis('off')\n",
    "\n",
    "### Uncomment the following lines of code if you would like to visualize a set of pairwise class \n",
    "### interpolations for a collection of different numbers, all in a single grid of interpolations.\n",
    "### You'll also see another visualization like this in the next code block!\n",
    "# plot_numbers = [2, 3, 4, 5, 7]\n",
    "# n_numbers = len(plot_numbers)\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# for i, first_plot_number in enumerate(plot_numbers):\n",
    "#     for j, second_plot_number in enumerate(plot_numbers):\n",
    "#         plt.subplot(n_numbers, n_numbers, i * n_numbers + j + 1)\n",
    "#         interpolate_class(first_plot_number, second_plot_number)\n",
    "#         plt.axis('off')\n",
    "# plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n",
    "# plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lczqHmy9mBHx"
   },
   "source": [
    "#### Changing the Noise Vector\n",
    "Now, what happens if you hold the class constant, but instead you change the noise vector? You can also interpolate the noise vector and generate an image at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r8ACKtedmJny"
   },
   "outputs": [],
   "source": [
    "n_interpolation = 9 # How many intermediate images you want + 2 (for the start and end image)\n",
    "\n",
    "# This time you're interpolating between the noise instead of the labels\n",
    "interpolation_label = get_one_hot_labels(torch.Tensor([5]).long(), n_classes).repeat(n_interpolation, 1).float()\n",
    "\n",
    "def interpolate_noise(first_noise, second_noise):\n",
    "    # This time you're interpolating between the noise instead of the labels\n",
    "    percent_first_noise = torch.linspace(0, 1, n_interpolation)[:, None].to(device)\n",
    "    interpolation_noise = first_noise * percent_first_noise + second_noise * (1 - percent_first_noise)\n",
    "\n",
    "    # Combine the noise and the labels again\n",
    "    noise_and_labels = combine_vectors(interpolation_noise, interpolation_label.to(device))\n",
    "    fake = gen(noise_and_labels)\n",
    "    show_tensor_images(fake, num_images=n_interpolation, nrow=int(math.sqrt(n_interpolation)), show=False)\n",
    "\n",
    "# Generate noise vectors to interpolate between\n",
    "### Change me! ###\n",
    "n_noise = 5 # Choose the number of noise examples in the grid\n",
    "plot_noises = [get_noise(1, z_dim, device=device) for i in range(n_noise)]\n",
    "plt.figure(figsize=(8, 8))\n",
    "for i, first_plot_noise in enumerate(plot_noises):\n",
    "    for j, second_plot_noise in enumerate(plot_noises):\n",
    "        plt.subplot(n_noise, n_noise, i * n_noise + j + 1)\n",
    "        interpolate_noise(first_plot_noise, second_plot_noise)\n",
    "        plt.axis('off')\n",
    "plt.subplots_adjust(top=1, bottom=0, left=0, right=1, hspace=0.1, wspace=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
